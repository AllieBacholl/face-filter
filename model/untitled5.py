# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wqm-S-kmJVvBaxWNJ48Q0SJs1QkTdSph
"""

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.quantization

class TinyTrafficSignCNN(nn.Module):
    def __init__(self, num_classes=43):
        super(TinyTrafficSignCNN, self).__init__()

        # Feature extractor
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=False)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1, bias=False)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2)

        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(2)

        # Classifier
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(32 * 4 * 4, 64, bias=False)
        self.relu_fc = nn.ReLU()
        self.fc2 = nn.Linear(64, num_classes, bias=False)

        # This enables quantization
        self.quant = torch.quantization.QuantStub()
        self.dequant = torch.quantization.DeQuantStub()

    def forward(self, x):
        x = self.quant(x)

        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)

        x = self.conv3(x)
        x = self.relu3(x)
        x = self.pool3(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu_fc(x)
        x = self.fc2(x)

        x = self.dequant(x)
        return x

    def fuse_model(self):
        # Fuse Conv + ReLU layers (in-place)
        torch.quantization.fuse_modules(self, [['conv1', 'relu1'],
                                               ['conv2', 'relu2'],
                                               ['conv3', 'relu3'],
                                               ['fc1', 'relu_fc']], inplace=True)



import json
import os
import torch
from torchvision.datasets import VisionDataset
from torchvision.datasets.utils import check_integrity, download_and_extract_archive


class LISA(VisionDataset):
    base_folder = 'lisa-batches'
    url = "https://github.com/AminJun/lisa/releases/download/v1/lisa.tar.gz"

    zipped = {
        'filename': 'lisa.tar.gz',
        'md5': 'd3e7bd49dc55c2d9240d4b5473848dcb',
    }

    label_file = 'labels.tensor'
    meta_file = 'meta.js'
    images_list = ['images_0.tensor', 'images_1.tensor', 'images_2.tensor']

    checksum = {
        'images_0.tensor': 'ac59f173c4d374859e73be64cee9de41',
        'images_1.tensor': '13df95c1f3b05fc9a90a83cb0febe50f',
        'images_2.tensor': '235f29c99e67019b1ba47dfe2492b461',
        label_file: 'a68f3549adbf898b26f1ab76ab515d38',
        meta_file: 'c52f0f118ff7e03c366608f7ea960d8f',
    }

    def _get_path(self, file: str) -> str:
        return os.path.join(self.root, self.base_folder, file)

    def __init__(self, root, train: bool, download=False, transform=None, target_transform=None):
        super(LISA, self).__init__(root=root, transform=transform, target_transform=target_transform)

        if download:
            self.download()

        if not self._check_integrity():
            raise RuntimeError('Dataset not found or corrupted. You can use download=True to download it')

        self.images = torch.cat([torch.load(self._get_path(file)) for file in self.images_list], 0)
        self.labels = torch.load(self._get_path(self.label_file))
        self._load_meta()

        self.train = train
        self._train_test_split()

    def _load_meta(self):
        with open(self._get_path(self.meta_file), 'r') as file:
            data = json.load(file)
            self.classes = data['classes']
            self.class_to_idx = data['name_to_label']

    def __getitem__(self, index) -> (torch.tensor, torch.tensor):
        """
        Args:
            index (int): Index

        Returns:
            tuple: (image, target) where target is index of the target class.
        """
        img, target = self.images[index], self.labels[index]
        img = img if self.transform is None else self.transform(img)
        target = target if self.target_transform is None else self.target_transform(target)
        return img, target

    def __len__(self) -> int:
        return len(self.images)

    def _check_integrity(self) -> bool:
        return all(check_integrity(self._get_path(filename), md5) for filename, md5 in self.checksum.items())

    def download(self):
        if self._check_integrity():
            print('Files already downloaded and verified')
            return
        download_and_extract_archive(self.url, self.root, **self.zipped)

    def extra_repr(self) -> str:
        return "No Split Yet"

    def _train_test_split(self, test_percent: float = 0.16):
        classes = {}
        for i, cl in enumerate(self.labels.numpy()):
            arr = classes.get(cl, [])
            arr.append(i)
            classes[cl] = arr

        train, test = [], []
        for cl, arr in classes.items():
            split_index = int(len(arr) * test_percent)
            test = test + arr[:split_index]
            train = train + arr[split_index:]

        sub = train if self.train else test
        self.images, self.labels = self.images[sub], self.labels[sub]

dataset = LISA(root='/content/', download=True, train=True)

# !wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Training_Images.zip
# !unzip GTSRB_Final_Training_Images.zip

# prompt: print the length of LISA

from torch.utils.data import DataLoader
from torchvision.transforms import ToPILImage

print("Num classes:", len(dataset.classes))

def pil_collate(batch):
    imgs, targets = zip(*batch)        # both are tuples of length batch_size
    return list(imgs), torch.tensor(targets)

dataset = LISA(
    root='/content/',
    download=False,
    train=True,
    transform=ToPILImage()            # now returns PIL
)

loader = DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    collate_fn=pil_collate
)

images, labels = next(iter(loader))  # images: list of 8 PIL.Image.Image
# now you can convert each on the fly:
import matplotlib.pyplot as plt
for img, lbl in zip(images, labels):
    plt.figure(figsize=(2.5,2.5))
    plt.imshow(img)
    plt.title(dataset.classes[lbl.item()])
    plt.axis('off')
    plt.show()

from torchvision import transforms, datasets
from torch.utils.data import DataLoader, random_split
import torch
from torch.nn.functional import one_hot

# 1) make a transform that starts with ToPILImage
data_transform = transforms.Compose([
    transforms.ToPILImage(),                       # <-- turn the tensor â†’ PIL
    transforms.Grayscale(num_output_channels=1),   # now acts on a PIL
    transforms.Resize((32, 32)),                   # PIL resize
    transforms.ToTensor(),                         # back to FloatTensor [0,1]
    transforms.Normalize((0.5,), (0.5,)),          # normalize the tensor
])

# 2) build the LISA dataset with that pipeline
full_dataset = LISA(
    root='/content/',
    download=False,
    train=True,
    transform=data_transform,
    #target_transform=lambda t: one_hot(t, 47).float()
)

# 3) split into train / val / test
total     = len(full_dataset)
train_sz  = int(0.70 * total)
val_sz    = int(0.15 * total)
test_sz   = total - train_sz - val_sz

train_ds, val_ds, test_ds = random_split(
    full_dataset,
    [train_sz, val_sz, test_sz],
    generator=torch.Generator().manual_seed(42)
)

# 4) DataLoaders (no custom collate needed now)
batch_size = 64
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)

# 5) verify
print(f" Train: {len(train_ds)}  Val: {len(val_ds)}  Test: {len(test_ds)}")
images, labels = next(iter(train_loader))
print("Image batch shape:", images.shape)   # [64, 1, 32, 32]
print("Label batch shape:", labels.shape)   # [64]

# prompt: train the model for 10 epochs with evaluation

import torch.optim as optim
import torch.nn.functional as F


# Check for GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initialize the model, loss function, and optimizer
model = TinyTrafficSignCNN(num_classes=47).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 20

for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Evaluation after each epoch
    model.eval() # Set the model to evaluation mode
    correct = 0
    total = 0
    with torch.no_grad():
        for data in val_loader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = 100 * correct / total

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

print("Finished Training")

# prompt: create a dataset and dataloader where the label of the image in class_#NUM subdirectory of saved_images is #NUM - 1

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
# !unzip saved_images.zip
class CustomImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []

        for class_num in range(1, len(os.listdir(image_dir)) + 1):  # Iterate through class directories
          class_dir = os.path.join(image_dir, f'class_{class_num}')
          if os.path.exists(class_dir):
            for filename in os.listdir(class_dir):
                if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add more extensions if needed
                    image_path = os.path.join(class_dir, filename)
                    self.image_paths.append(image_path)
                    self.labels.append(class_num -1)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path)
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Example usage:
image_dir = '/content/saved_images'  # Replace with the actual directory
transform = transforms.Compose([
    transforms.Resize((32, 32)),  # Resize the images
    transforms.ToTensor(),  # Convert to tensor
    transforms.Normalize((0.5, ), (0.5, ))  # Normalize
])


dataset = CustomImageDataset(image_dir, transform=transform)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Iterate through the dataloader
for images, labels in dataloader:
    print("Image batch shape:", images.shape)
    print("Label batch shape:", labels.shape)
    # Access individual images and labels
    # ...
    break # Show only one batch

# prompt: fine tune (still adjust the model) linear layers the model on saved_images dataset and calculate accuracy

# Fine-tuning the linear layers
for param in model.parameters():
    param.requires_grad = False

for param in model.fc1.parameters():
    param.requires_grad = True
for param in model.fc2.parameters():
    param.requires_grad = True

optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001) # Lower learning rate for fine-tuning

# Fine-tuning loop (adjust num_epochs as needed)
num_epochs = 5  # Example: 5 epochs for fine-tuning

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for i, data in enumerate(dataloader, 0): # Use the custom dataloader
        inputs, labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Evaluation on the custom dataset after each epoch
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in dataloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = 100 * correct / total

    print(f"Fine-tuning Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

print("Finished Fine-tuning")

# prompt: print an example image and its label

import matplotlib.pyplot as plt
import numpy as np

# ... (your existing code) ...

# Example usage: Access a batch of data and display an image
for images, labels in train_loader:
    # Choose an image from the batch (e.g., the first one)
    image_index = 0
    image = images[image_index]
    label = labels[image_index]

    # Convert the tensor to a NumPy array and transpose it
    image_np = image.numpy().transpose(1, 2, 0)

    # Reverse the normalization
    image_np = (image_np * 0.5) + 0.5

    # Display the image
    plt.imshow(image_np, cmap='gray')  # Use 'gray' for grayscale images
    plt.title(f"Label: {label}")
    plt.show()

    break # Show only one image



# prompt: static quantize the model and evaluate its loss and display a few results of the actual image, true label/predicted label

import matplotlib.pyplot as plt
# Fuse the model
model.fuse_model()

# Specify quantization configuration
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

#Calibrate the model with a few batches of data
num_calibration_batches = 10
for images, _ in train_loader:
    model(images.to(device))
    num_calibration_batches -= 1
    if num_calibration_batches <= 0:
        break

# Convert the model to quantized form
torch.quantization.convert(model, inplace=True)


# Evaluate the quantized model
model.eval()
quantized_loss = 0.0
correct_predictions = 0
total_samples = 0

with torch.no_grad():
  for images, labels in test_loader:
    images, labels = images.to(device), labels.to(device)
    outputs = model(images)
    loss = criterion(outputs, labels)
    quantized_loss += loss.item()

    _, predicted = torch.max(outputs, 1)
    total_samples += labels.size(0)
    correct_predictions += (predicted == labels).sum().item()

average_quantized_loss = quantized_loss / len(test_loader)
accuracy = 100 * correct_predictions / total_samples

print(f"Quantized Model Loss: {average_quantized_loss:.4f}")
print(f"Quantized Model Accuracy: {accuracy:.2f}%")


# Display a few results
num_results_to_display = 5
with torch.no_grad():
    for i, (images, labels) in enumerate(test_loader):
        if i >= num_results_to_display:
            break

        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        for j in range(min(images.size(0), 3)): # Limit to 3 images per batch for display
            image_np = images[j].cpu().numpy().transpose(1, 2, 0)
            image_np = (image_np * 0.5) + 0.5

            plt.imshow(image_np, cmap='gray')
            plt.title(f"True: {labels[j].item()}, Predicted: {predicted[j].item()}")
            plt.show()

# prompt: evaluate model on saved images and show a few results

import matplotlib.pyplot as plt
# ... (your existing code) ...

# Display a few results
num_results_to_display = 5
with torch.no_grad():
    for i, (images, labels) in enumerate(dataloader):
        if i >= num_results_to_display:
            break

        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        for j in range(min(images.size(0), 3)): # Limit to 3 images per batch for display
            image_np = images[j].cpu().numpy().transpose(1, 2, 0)
            image_np = (image_np * 0.5) + 0.5

            plt.imshow(image_np, cmap='gray')
            plt.title(f"True: {labels[j].item()}, Predicted: {predicted[j].item()}")
            plt.show()

# prompt: report accuracy on saved images

# Evaluation on the custom dataset
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for data in dataloader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f"Accuracy on saved images: {accuracy:.2f}%")

# prompt: Save model

# Save the quantized model
torch.save(model.state_dict(), 'quantized_traffic_sign_model.pth')

# prompt: save the model and print quantized model size and structure

import os

# Save the quantized model
quantized_model_path = 'quantized_model.pth'
torch.save(model.state_dict(), quantized_model_path)

# Print the size of the saved model
quantized_model_size = os.path.getsize(quantized_model_path)
print(f"Quantized model size: {quantized_model_size / (1024*1024):.2f} MB")

# Print the model structure
model

# prompt: LOAD MODEL and do inference

# Load the quantized model
model = TinyTrafficSignCNN().to(device)  # Assuming 'device' is defined as before
model.fuse_model()

model.qconfig = torch.quantization.get_default_qconfig('qnnpack')
torch.quantization.prepare(model, inplace=True)
# â€¦run calibration batchesâ€¦
torch.quantization.convert(model, inplace=True)

model.load_state_dict(torch.load('quantized_traffic_sign_model.pth'))
model.eval()

# Display a few results
num_results_to_display = 5
with torch.no_grad():
    for i, (images, labels) in enumerate(test_loader):
        if i >= num_results_to_display:
            break

        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        for j in range(min(images.size(0), 3)): # Limit to 3 images per batch for display
            image_np = images[j].cpu().numpy().transpose(1, 2, 0)
            image_np = (image_np * 0.5) + 0.5

            plt.imshow(image_np, cmap='gray')
            plt.title(f"True: {labels[j].item()}, Predicted: {predicted[j].item()}")
            plt.show()

from PIL import Image
def preprocess_image(image_path, show=True):
    transform = transforms.Compose([
        transforms.Grayscale(num_output_channels=1),
        transforms.Resize((32, 32)),
        transforms.ToTensor(),  # Converts to [C, H, W] and scales to [0, 1]
        transforms.Normalize((0.5,), (0.5,)), # Normalize
    ])

    image = Image.open(image_path).convert('L')
    tensor = transform(image).unsqueeze(0)  # Shape: [1, 1, 32, 32]

    import os
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

    # Show the image using matplotlib
    if show:
        plt.imshow(tensor[0][0].numpy(), cmap='gray')
        plt.title("Transformed Input Image")
        plt.axis('off')
        plt.show()

    return tensor

model(preprocess_image("/content/output_image.jpg"))

# prompt: print an image of class index 0 and randomize your result

import random
import matplotlib.pyplot as plt

# Assuming 'test_loader' and 'model' are defined as in your original code

# Select a random image from the test dataset with class index 0
class_index_to_find = 0
found_image = False

for images, labels in test_loader:
    for i in range(images.shape[0]):
        if labels[i] == class_index_to_find:
            image = images[i]
            label = labels[i]
            found_image = True
            break
    if found_image:
        break

if found_image:
    # Convert the tensor to a NumPy array and transpose it
    image_np = image.cpu().numpy().transpose(1, 2, 0)

    # Reverse the normalization
    image_np = (image_np * 0.5) + 0.5

    # Display the image
    plt.imshow(image_np, cmap='gray')
    plt.title(f"Class Index: {label.item()}")  # Display the class index as the title
    plt.show()

else:
  print(f"No image found with class index {class_index_to_find}")

# prompt: Print the all labels of the corresponding class indices

# Assuming 'dataset' is your LISA dataset object
for i in range(len(dataset.classes)):
    print(f"Class Index: {i}, Label: {dataset.classes[i]}")

# prompt: print an image of pedestrian crossing

import matplotlib.pyplot as plt
# Assuming 'dataset' and 'test_loader' are defined as in your original code

# Find an image with class index 14 (pedestrian crossing)
class_index_to_find = 39
found_image = False

for images, labels in test_loader:
    for i in range(images.shape[0]):
        if labels[i] == class_index_to_find:
            image = images[i]
            label = labels[i]
            found_image = True
            break
    if found_image:
        break

if found_image:
    # Convert the tensor to a NumPy array and transpose it
    image_np = image.cpu().numpy().transpose(1, 2, 0)

    # Reverse the normalization
    image_np = (image_np * 0.5) + 0.5

    # Display the image
    plt.imshow(image_np, cmap='gray')
    plt.title(f"Class Index: {label.item()}, Label: {dataset.classes[label.item()]}")  # Display class name
    plt.show()
else:
    print(f"No image found with class index {class_index_to_find}")

# 1. Load the original model (not converted yet)
# model = TinyTrafficSignCNN().to(device)
# model.load_state_dict(torch.load('/content/quantized_traffic_sign_model (3).pth'))
# model.eval()

# 2. Fine-tune using saved_image dataset
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in dataloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}")

print("âœ… Fine-tuning complete.")

# 3. Now quantize (post-training quantization)
model.eval()
model.fuse_model()  # Only if your model supports it

model.qconfig = torch.quantization.get_default_qconfig('qnnpack')
torch.quantization.prepare(model, inplace=True)

# 4. Calibration step â€” run some batches through model
with torch.no_grad():
    for inputs, _ in dataloader:
        model(inputs.to(device))

torch.quantization.convert(model, inplace=True)

# 5. Save the quantized fine-tuned model
torch.save(model.state_dict(), 'fine_tuned_quantized_model.pth')